# @package _global_

# to execute this experiment run:
# python train.py experiment=example

# defaults:
#   - _self_

trainer:
  min_epochs: 1
  max_epochs: 3
  gradient_clip_val: 0.5
  log_every_n_steps: 10

model:
  optimizer:
    lr: 0.005
  compile: false

data:
  batch_size: 4
  num_workers: 2
  pin_memory: true

task_name: "train"
seed: 66
tags: ["llm"]
